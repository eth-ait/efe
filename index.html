<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EFE:End-to-end Frame-to-Gaze Estimation">
  <meta name="keywords" content="ldmoa">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EFE:End-to-end Frame-to-Gaze Estimation</title>

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-72PW1FZDE4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-72PW1FZDE4');
  </script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EFE:End-to-end Frame-to-Gaze Estimation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/haldun-balÄ±m/">Haldun Balim</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://swook.net">Seonwook Park</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://xiwang1212.github.io/homepage/">Xi Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.ccmitss.com/zhang">Xucong Zhang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/hilliges">Otmar Hilliges</a><sup>1</sup>,</span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ETH Zurich</span>
            <span class="author-block"><sup>2</sup>Lunit Inc.</span>
            <span class="author-block"><sup>3</sup>TU Delft</span>
            <br>              
            <span class="author-block">  <b>GAZE 2023, CVPR</b></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2023W/GAZE/html/Balim_EFE_End-to-End_Frame-To-Gaze_Estimation_CVPRW_2023_paper.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--               <span class="link-block">
                <a href="https://youtu.be/YB1_xKlueUI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
          </div>
        </div>
      </div>
    </div>
</section>

<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <div class="diagram">
        <img src="static/images/teaser.png" alt="Overview" height="750" width="1000" />
    </div>
  </div>
</div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           Despite the recent development of learning-based gaze estimation methods, most methods require one or more eye or face region crops as inputs and produce a gaze direction vector as output.
Cropping results in a higher resolution in the eye regions and having fewer confounding factors (such as clothing and hair) is believed to benefit the final model performance. However, this eye/face patch cropping process is expensive, erroneous, and implementation-specific for different methods. In this paper, we propose a frame-to-gaze network that directly predicts both 3D gaze origin and 3D gaze direction from the raw frame out of the camera without any face or eye cropping. Our method demonstrates that direct gaze regression from the raw downscaled frame, from FHD/HD to VGA/HVGA resolution, is possible despite the challenges of having very few pixels in the eye region. The proposed method achieves comparable results to state-of-the-art methods in Point-of-Gaze (PoG) estimation on three public gaze datasets: GazeCapture, MPIIFaceGaze, and EVE, and generalizes well to extreme camera view changes. 
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section_architecture">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Architecture</h2>
        <div class="diagram">
        <img src="static/images/architecture.png" alt="Overview" height="750" width="1000" />
      </div>
        <h2 class="subtitle has-text-centered">
            We present our tailored end-to-end architecture for frame-to-gaze estimation problem.
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Poster</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="diagram">
            <img src="static/images/ldmoa_poster.png" alt="Overview" height="750" width="1000" />
        </div>
      </div>
    </div>
  </div>
</div> -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{Balim_2023_CVPR,
    author    = {Balim, Haldun and Park, Seonwook and Wang, Xi and Zhang, Xucong and Hilliges, Otmar},
    title     = {EFE: End-to-End Frame-To-Gaze Estimation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2023},
    pages     = {2687-2696}
}</code></pre>
  </div>
</section>

</body>
</html>
